{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown data querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From minicuber class\n",
    "\n",
    "from pyproj import Transformer\n",
    "from pyproj.aoi import AreaOfInterest\n",
    "from pyproj.database import query_utm_crs_info\n",
    "\n",
    "def bbox(lon_lat, xy_shape, resolution):\n",
    "\n",
    "    utm_epsg = int(query_utm_crs_info(\n",
    "        datum_name=\"WGS 84\",\n",
    "        area_of_interest=AreaOfInterest(lon_lat[0], lon_lat[1], lon_lat[0], lon_lat[1])\n",
    "    )[0].code)\n",
    "\n",
    "    transformer = Transformer.from_crs(4326, utm_epsg, always_xy=True)\n",
    "\n",
    "    x_center, y_center = transformer.transform(*lon_lat)\n",
    "\n",
    "    nx, ny = xy_shape\n",
    "    \n",
    "    x_left, x_right = x_center - resolution * (nx//2), x_center + resolution * (nx//2)\n",
    "\n",
    "    y_top, y_bottom = y_center + resolution * (ny//2), y_center - resolution * (ny//2)\n",
    "\n",
    "    return transformer.transform_bounds(x_left, y_bottom, x_right, y_top, direction = 'INVERSE') # left, bottom, right, top\n",
    "\n",
    "def padded_bbox(bbox, xy_shape):\n",
    "    left, bottom, right, top = bbox\n",
    "    lat_extra = (top - bottom) / xy_shape[0] * 6\n",
    "    lon_extra = (right - left) / xy_shape[1] * 6\n",
    "    return left - lon_extra, bottom - lat_extra, right + lon_extra, top + lat_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sentinel2.py\n",
    "\n",
    "import os\n",
    "import pystac_client\n",
    "from pystac_client.stac_api_io import StacApiIO\n",
    "\n",
    "URL = 'https://planetarycomputer.microsoft.com/api/stac/v1'\n",
    "\n",
    "stac_api_io = StacApiIO()\n",
    "stac_api_io.session.verify = os.environ.get('CURL_CA_BUNDLE')\n",
    "catalog = pystac_client.Client.open(URL,stac_io=stac_api_io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From minicuber.load_data and sentinel2\n",
    "\n",
    "lon_lat = (7.7256, 46.9206) #lon-lat\n",
    "xy_shape = (128, 128)\n",
    "resolution = 10\n",
    "time_interval = \"2022-01-01/2022-01-31\"\n",
    "\n",
    "bbox_query = bbox(lon_lat=lon_lat, xy_shape=xy_shape, resolution=resolution)\n",
    "padded_bbox_query = padded_bbox(bbox_query, xy_shape)\n",
    "\n",
    "search = catalog.search(\n",
    "        bbox = padded_bbox_query,\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        datetime=time_interval\n",
    ") # returns a ItermSearch object (search object of STAC API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sentinel2\n",
    "\n",
    "import planetary_computer as pc \n",
    "import stackstac\n",
    "import rasterio\n",
    "from rasterio import RasterioIOError\n",
    "\n",
    "items_s2 = pc.sign(search) # will perform serach and return ItemCollection that is signed. GeoJSON FeatureCollection whose features are all STAC Items\n",
    "\n",
    "metadata = items_s2.to_dict()['features'][0][\"properties\"]\n",
    "epsg = metadata[\"proj:epsg\"]\n",
    "\n",
    "gdal_session = stackstac.DEFAULT_GDAL_ENV.updated(always=dict(session=rasterio.session.AWSSession(aws_unsigned = True, endpoint_url = None)))\n",
    "bands = [\"AOT\", \"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"WVP\"]\n",
    "stack = stackstac.stack(items_s2, epsg = epsg, assets = bands, dtype = \"float32\", properties = [\"s2:product_uri\", \"s2:mean_solar_zenith\", \"s2:mean_solar_azimuth\"], band_coords = False, bounds_latlon = padded_bbox_query, xy_coords = 'center', chunksize = 2048,errors_as_nodata=(RasterioIOError('.*'), ), gdal_env=gdal_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = stack.band.values\n",
    "stack[\"band\"] = [f\"s2_{b}\" for b in stack.band.values]\n",
    "\n",
    "stack = stack.to_dataset(\"band\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import urllib\n",
    "import uuid\n",
    "from xml.dom import minidom\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def parse_MTD_TL(in_file):\n",
    "    \"\"\"\n",
    "    Parses the MTD_TL.xml metadata file provided by ESA.This metadata\n",
    "    XML is usually placed in the GRANULE subfolder of a ESA-derived\n",
    "    S2 product and named 'MTD_TL.xml'.\n",
    "\n",
    "    The 'MTD_TL.xml' is available for both processing levels (i.e.,\n",
    "    L1C and L2A). The function is able to handle both processing\n",
    "    sources and returns some entries available in L2A processing level,\n",
    "    only, as None type objects.\n",
    "\n",
    "    The function extracts the most important metadata from the XML and\n",
    "    returns a dict with those extracted entries.\n",
    "\n",
    "    :param in_file:\n",
    "        filepath of the scene metadata xml 8MTD_TL.xml)\n",
    "    :return metadata:\n",
    "        dict with extracted metadata entries\n",
    "    \"\"\"\n",
    "    # parse the xml file into a minidom object\n",
    "    xmldoc = minidom.parse(in_file)\n",
    "\n",
    "    # now, the values of some relevant tags can be extracted:\n",
    "    metadata = dict()\n",
    "\n",
    "    # get tile ID of L2A product and its corresponding L1C counterpart\n",
    "    tile_id_xml = xmldoc.getElementsByTagName(\"TILE_ID\")\n",
    "    # adaption to older Sen2Cor version\n",
    "    check_l1c = True\n",
    "    if len(tile_id_xml) == 0:\n",
    "        tile_id_xml = xmldoc.getElementsByTagName(\"TILE_ID_2A\")\n",
    "        check_l1c = False\n",
    "    tile_id = tile_id_xml[0].firstChild.nodeValue\n",
    "    scene_id = tile_id.split(\".\")[0]\n",
    "    metadata[\"SCENE_ID\"] = scene_id\n",
    "\n",
    "    # check if the scene is L1C or L2A\n",
    "    is_l1c = False\n",
    "    if check_l1c:\n",
    "        try:\n",
    "            l1c_tile_id_xml = xmldoc.getElementsByTagName(\"L1C_TILE_ID\")\n",
    "            l1c_tile_id = l1c_tile_id_xml[0].firstChild.nodeValue\n",
    "            l1c_tile_id = l1c_tile_id.split(\".\")[0]\n",
    "            metadata[\"L1C_TILE_ID\"] = l1c_tile_id\n",
    "        except Exception:\n",
    "            logger.info(f\"{scene_id} is L1C processing level\")\n",
    "            is_l1c = True\n",
    "\n",
    "    # sensing time (acquisition time)\n",
    "    sensing_time_xml = xmldoc.getElementsByTagName(\"SENSING_TIME\")\n",
    "    sensing_time = sensing_time_xml[0].firstChild.nodeValue\n",
    "    metadata[\"SENSING_TIME\"] = sensing_time\n",
    "    metadata[\"SENSING_DATE\"] = datetime.strptime(\n",
    "        sensing_time.split(\"T\")[0], \"%Y-%m-%d\"\n",
    "    ).date()\n",
    "\n",
    "    # number of rows and columns for each resolution -> 10, 20, 60 meters\n",
    "    nrows_xml = xmldoc.getElementsByTagName(\"NROWS\")\n",
    "    ncols_xml = xmldoc.getElementsByTagName(\"NCOLS\")\n",
    "    resolutions = [\"_10m\", \"_20m\", \"_60m\"]\n",
    "    # order: 10, 20, 60 meters spatial resolution\n",
    "    for ii in range(3):\n",
    "        nrows = nrows_xml[ii].firstChild.nodeValue\n",
    "        ncols = ncols_xml[ii].firstChild.nodeValue\n",
    "        metadata[\"NROWS\" + resolutions[ii]] = int(nrows)\n",
    "        metadata[\"NCOLS\" + resolutions[ii]] = int(ncols)\n",
    "\n",
    "    # EPSG-code\n",
    "    epsg_xml = xmldoc.getElementsByTagName(\"HORIZONTAL_CS_CODE\")\n",
    "    epsg = epsg_xml[0].firstChild.nodeValue\n",
    "    metadata[\"EPSG\"] = int(epsg.split(\":\")[1])\n",
    "\n",
    "    # Upper Left Corner coordinates -> is the same for all three resolutions\n",
    "    ulx_xml = xmldoc.getElementsByTagName(\"ULX\")\n",
    "    uly_xml = xmldoc.getElementsByTagName(\"ULY\")\n",
    "    ulx = ulx_xml[0].firstChild.nodeValue\n",
    "    uly = uly_xml[0].firstChild.nodeValue\n",
    "    metadata[\"ULX\"] = float(ulx)\n",
    "    metadata[\"ULY\"] = float(uly)\n",
    "    # endfor\n",
    "\n",
    "    # extract the mean zenith and azimuth angles\n",
    "    # the sun angles come first followed by the mean angles per band\n",
    "    zenith_angles = xmldoc.getElementsByTagName(\"ZENITH_ANGLE\")\n",
    "    metadata[\"SUN_ZENITH_ANGLE\"] = float(zenith_angles[0].firstChild.nodeValue)\n",
    "\n",
    "    azimuth_angles = xmldoc.getElementsByTagName(\"AZIMUTH_ANGLE\")\n",
    "    metadata[\"SUN_AZIMUTH_ANGLE\"] = float(azimuth_angles[0].firstChild.nodeValue)\n",
    "\n",
    "    # get the mean zenith and azimuth angle over all bands\n",
    "    sensor_zenith_angles = [float(x.firstChild.nodeValue) for x in zenith_angles[1::]]\n",
    "    metadata[\"SENSOR_ZENITH_ANGLE\"] = np.mean(np.asarray(sensor_zenith_angles))\n",
    "\n",
    "    sensor_azimuth_angles = [float(x.firstChild.nodeValue) for x in azimuth_angles[1::]]\n",
    "    metadata[\"SENSOR_AZIMUTH_ANGLE\"] = np.mean(np.asarray(sensor_azimuth_angles))\n",
    "\n",
    "    # extract scene relevant data about nodata values, cloud coverage, etc.\n",
    "    cloudy_xml = xmldoc.getElementsByTagName(\"CLOUDY_PIXEL_PERCENTAGE\")\n",
    "    cloudy = cloudy_xml[0].firstChild.nodeValue\n",
    "    metadata[\"CLOUDY_PIXEL_PERCENTAGE\"] = float(cloudy)\n",
    "\n",
    "    degraded_xml = xmldoc.getElementsByTagName(\"DEGRADED_MSI_DATA_PERCENTAGE\")\n",
    "    degraded = degraded_xml[0].firstChild.nodeValue\n",
    "    metadata[\"DEGRADED_MSI_DATA_PERCENTAGE\"] = float(degraded)\n",
    "\n",
    "    # the other tags are available in L2A processing level, only\n",
    "    if not is_l1c:\n",
    "        nodata_xml = xmldoc.getElementsByTagName(\"NODATA_PIXEL_PERCENTAGE\")\n",
    "        nodata = nodata_xml[0].firstChild.nodeValue\n",
    "        metadata[\"NODATA_PIXEL_PERCENTAGE\"] = float(nodata)\n",
    "\n",
    "        darkfeatures_xml = xmldoc.getElementsByTagName(\"DARK_FEATURES_PERCENTAGE\")\n",
    "        darkfeatures = darkfeatures_xml[0].firstChild.nodeValue\n",
    "        metadata[\"DARK_FEATURES_PERCENTAGE\"] = float(darkfeatures)\n",
    "\n",
    "        cs_xml = xmldoc.getElementsByTagName(\"CLOUD_SHADOW_PERCENTAGE\")\n",
    "        cs = cs_xml[0].firstChild.nodeValue\n",
    "        metadata[\"CLOUD_SHADOW_PERCENTAGE\"] = float(cs)\n",
    "\n",
    "        veg_xml = xmldoc.getElementsByTagName(\"VEGETATION_PERCENTAGE\")\n",
    "        veg = veg_xml[0].firstChild.nodeValue\n",
    "        metadata[\"VEGETATION_PERCENTAGE\"] = float(veg)\n",
    "\n",
    "        noveg_xml = xmldoc.getElementsByTagName(\"NOT_VEGETATED_PERCENTAGE\")\n",
    "        noveg = noveg_xml[0].firstChild.nodeValue\n",
    "        metadata[\"NOT_VEGETATED_PERCENTAGE\"] = float(noveg)\n",
    "\n",
    "        water_xml = xmldoc.getElementsByTagName(\"WATER_PERCENTAGE\")\n",
    "        water = water_xml[0].firstChild.nodeValue\n",
    "        metadata[\"WATER_PERCENTAGE\"] = float(water)\n",
    "\n",
    "        unclass_xml = xmldoc.getElementsByTagName(\"UNCLASSIFIED_PERCENTAGE\")\n",
    "        unclass = unclass_xml[0].firstChild.nodeValue\n",
    "        metadata[\"UNCLASSIFIED_PERCENTAGE\"] = float(unclass)\n",
    "\n",
    "        cproba_xml = xmldoc.getElementsByTagName(\"MEDIUM_PROBA_CLOUDS_PERCENTAGE\")\n",
    "        cproba = cproba_xml[0].firstChild.nodeValue\n",
    "        metadata[\"MEDIUM_PROBA_CLOUDS_PERCENTAGE\"] = float(cproba)\n",
    "\n",
    "        hcproba_xml = xmldoc.getElementsByTagName(\"HIGH_PROBA_CLOUDS_PERCENTAGE\")\n",
    "        hcproba = hcproba_xml[0].firstChild.nodeValue\n",
    "        metadata[\"HIGH_PROBA_CLOUDS_PERCENTAGE\"] = float(hcproba)\n",
    "\n",
    "        thcirrus_xml = xmldoc.getElementsByTagName(\"THIN_CIRRUS_PERCENTAGE\")\n",
    "        thcirrus = thcirrus_xml[0].firstChild.nodeValue\n",
    "        metadata[\"THIN_CIRRUS_PERCENTAGE\"] = float(thcirrus)\n",
    "\n",
    "        snowice_xml = xmldoc.getElementsByTagName(\"SNOW_ICE_PERCENTAGE\")\n",
    "        snowice = snowice_xml[0].firstChild.nodeValue\n",
    "        metadata[\"SNOW_ICE_PERCENTAGE\"] = float(snowice)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "\n",
    "def angles_from_mspc(url):\n",
    "    \"\"\"\n",
    "    Extract viewing angles from MS Planetary Computer\n",
    "    metadata XML (this is a work-around until STAC provides the angles\n",
    "    directly)\n",
    "\n",
    "    :param url:\n",
    "        URL to the metadata XML file\n",
    "    :returns:\n",
    "        extracted angles as dictionary\n",
    "    \"\"\"\n",
    "    response = urllib.request.urlopen(pc.sign_url(url)).read()\n",
    "    temp_file = os.path.join(tempfile.gettempdir(), f'{uuid.uuid4()}.xml')\n",
    "    with open(temp_file, 'wb') as dst:\n",
    "        dst.write(response)\n",
    "    metadata = parse_MTD_TL(in_file=temp_file)\n",
    "    # get sensor zenith and azimuth angle\n",
    "    sensor_angles = ['SENSOR_ZENITH_ANGLE', 'SENSOR_AZIMUTH_ANGLE', 'SUN_ZENITH_ANGLE', 'SUN_AZIMUTH_ANGLE']\n",
    "    sensor_angle_dict = {\n",
    "        k: v for k, v in metadata.items() if k in sensor_angles}\n",
    "    return sensor_angle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster method to add the sensor angles\n",
    "\n",
    "items_dict = {item.id: item for item in items_s2}\n",
    "ordered_items = [items_dict[itemid] for itemid in stack.id.values]\n",
    "sensor_zenith = []\n",
    "for item in ordered_items:\n",
    "    item = item.to_dict()\n",
    "    granule_metadata_href = item[\"assets\"][\"granule-metadata\"][\"href\"].split('xml')[0] + 'xml'\n",
    "    sensor_angles = angles_from_mspc(granule_metadata_href)\n",
    "    sensor_zenith.append(sensor_angles[\"SENSOR_ZENITH_ANGLE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sensor angles using product uri\n",
    "\n",
    "# Move the extra coordinates to variables, as the time variable will be change to date\n",
    "stack = stack.reset_coords('s2:product_uri').rename({'s2:product_uri': 'product_uri'})\n",
    "stack = stack.reset_coords('s2:mean_solar_zenith').rename({'s2:mean_solar_zenith': 'mean_solar_zenith'})\n",
    "stack = stack.reset_coords('s2:mean_solar_azimuth').rename({'s2:mean_solar_azimuth': 'mean_solar_azimuth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View items -> they are whole tiles, at different dates\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "df = gpd.GeoDataFrame.from_features(items_s2.to_dict(), crs=\"epsg:4326\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "df.plot(ax=ax, edgecolor='red', facecolor='none')\n",
    "ax.set_xlim((df.total_bounds[0]-0.5, df.total_bounds[2]+0.5))\n",
    "ax.set_ylim((df.total_bounds[1]-0.1, df.total_bounds[3]+0.1))\n",
    "ctx.add_basemap(ax, crs=df.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying download approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out the tile geometry for tile 32TMT\n",
    "\n",
    "import planetary_computer as pc \n",
    "import stackstac\n",
    "import rasterio\n",
    "from rasterio import RasterioIOError\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pystac_client\n",
    "from pystac_client.stac_api_io import StacApiIO\n",
    "\n",
    "# Setup client\n",
    "\n",
    "URL = 'https://planetarycomputer.microsoft.com/api/stac/v1'\n",
    "\n",
    "stac_api_io = StacApiIO()\n",
    "stac_api_io.session.verify = os.environ.get('CURL_CA_BUNDLE')\n",
    "catalog = pystac_client.Client.open(URL,stac_io=stac_api_io)\n",
    "\n",
    "# Define what we search\n",
    "\n",
    "lon_lat = (8.5417, 47.3796) # Zurich, in 32TMT\n",
    "xy_shape = (128, 128)\n",
    "resolution = 10\n",
    "time_interval = \"2021-07-01/2021-07-31\"\n",
    "\n",
    "bbox_query = bbox(lon_lat=lon_lat, xy_shape=xy_shape, resolution=resolution)\n",
    "padded_bbox_query = padded_bbox(bbox_query, xy_shape)\n",
    "\n",
    "search = catalog.search(\n",
    "        bbox = padded_bbox_query,\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        datetime=time_interval\n",
    ") # retruns a ItermSearch object (search object of STAC API)\n",
    "\n",
    "\n",
    "# Query data\n",
    "\n",
    "items_s2 = pc.sign(search) # will perform serach and return ItemCollection that is signed. GeoJSON FeatureCollection whose features are all STAC Items\n",
    "\n",
    "metadata = items_s2.to_dict()['features'][0][\"properties\"]\n",
    "epsg = metadata[\"proj:epsg\"]\n",
    "\n",
    "gdal_session = stackstac.DEFAULT_GDAL_ENV.updated(always=dict(session=rasterio.session.AWSSession(aws_unsigned = True, endpoint_url = None)))\n",
    "bands = [\"AOT\", \"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"WVP\"]\n",
    "stack = stackstac.stack(items_s2, epsg = epsg, assets = bands, dtype = \"float32\", properties = [\"sentinel:product_id\"], band_coords = False, bounds_latlon = padded_bbox_query, xy_coords = 'center', chunksize = 2048,errors_as_nodata=(RasterioIOError('.*'), ), gdal_env=gdal_session)\n",
    "\n",
    "\n",
    "# View data\n",
    "df = gpd.GeoDataFrame.from_features(items_s2.to_dict(), crs=\"epsg:4326\")\n",
    "\n",
    "print(df.geometry.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query data\n",
    "\n",
    "specs = {\n",
    "    \"lon_lat\": (df.geometry.values[0].centroid.x, df.geometry.values[0].centroid.y), # center pixel (lon-lat)\n",
    "    \"xy_shape\": (1024, 1024), # width, height of cutout around center pixel\n",
    "    \"resolution\": 10, # in meters.. will use this on a local UTM grid..\n",
    "    \"time_interval\": \"2021-01-01/2021-01-31\",\n",
    "    \"providers\": [\n",
    "        {\n",
    "            \"name\": \"s2\",\n",
    "            \"kwargs\": {\"bands\": [\"AOT\", \"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"WVP\"], \"best_orbit_filter\": False, \"five_daily_filter\": False, \"brdf_correction\": True, \"cloud_mask\": False, \"aws_bucket\": \"planetary_computer\"}\n",
    "        }\n",
    "        ]\n",
    "}\n",
    "\n",
    "mc = emc.load_minicube(specs, compute = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare bbox that is queried and extent of data returned\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "bbox_query = bbox(lon_lat=specs[\"lon_lat\"], xy_shape=specs[\"xy_shape\"], resolution=specs[\"resolution\"])\n",
    "padded_bbox_query = padded_bbox(bbox_query, specs[\"xy_shape\"])\n",
    "\n",
    "\n",
    "# Create a Polygon from the coordinates\n",
    "polygon_bbox = Polygon([\n",
    "    (bbox_query[0], bbox_query[1]),\n",
    "    (bbox_query[2], bbox_query[1]),\n",
    "    (bbox_query[2], bbox_query[3]),\n",
    "    (bbox_query[0], bbox_query[3]),\n",
    "    (bbox_query[0], bbox_query[1])\n",
    "])\n",
    "\n",
    "polygon_padded_bbox = Polygon([\n",
    "    (padded_bbox_query[0], padded_bbox_query[1]),\n",
    "    (padded_bbox_query[2], padded_bbox_query[1]),\n",
    "    (padded_bbox_query[2], padded_bbox_query[3]),\n",
    "    (padded_bbox_query[0], padded_bbox_query[3]),\n",
    "    (padded_bbox_query[0], padded_bbox_query[1])\n",
    "])\n",
    "\n",
    "# Create a GeoDataFrame with the Polygon\n",
    "gdf_bbox = gpd.GeoDataFrame({'geometry': [polygon_bbox]}, crs='EPSG:4326')\n",
    "gdf_padded_bbox = gpd.GeoDataFrame({'geometry': [polygon_padded_bbox]}, crs='EPSG:4326')\n",
    "\n",
    "# Get extent of the data\n",
    "point_ul = (mc['lon'].values[0], mc['lat'].values[0])\n",
    "point_ur = (mc['lon'].values[-1], mc['lat'].values[0])\n",
    "\n",
    "# Plot the GeoDataFrame\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "gdf_bbox.plot(ax=ax, alpha=0.5, edgecolor='blue', facecolor='none')\n",
    "gdf_padded_bbox.plot(ax=ax, alpha=0.5, edgecolor='red', facecolor='none')\n",
    "plt.plot(point_ul[0], point_ul[1], 'ko', markersize=3)  \n",
    "plt.plot(point_ur[0], point_ur[1], 'ko', markersize=3)  \n",
    "\n",
    "\n",
    "# Add contextily background\n",
    "ctx.add_basemap(ax, crs=gdf_bbox.crs)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
